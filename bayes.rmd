---
graphics: yes
output: 
  pdf_document:
    fig_caption: true
    keep_tex: true
    latex_engine: pdflatex
    number_sections: true
    fig_height: 5
    fig_width: 10
header-includes:
- \usepackage[english]{babel}
- \usepackage{setspace}
- \usepackage{bm}
- \hyphenation{}
- \pagenumbering{arabic}
- \numberwithin{equation}{section}
- \newcommand{\E}{\mathbb{E}}
- \newcommand{\R}{\mathbb{R}}
- \newcommand{\argmax}{\text{argmax}}
fontsize: 10pt
geometry: a4paper, left=30mm, right=30mm, top=20mm, bottom=20mm, includefoot
bibliography: references.bib
documentclass: article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, echo=FALSE, warning=FALSE, message=FALSE}
#library(MASS)
#library(mixtools)
#library(plyr)
library(xtable)
library(mvtnorm)
```

<!-- Titlepage -->
\thispagestyle{empty}

\begin{raggedright}
  Freie Universität Berlin\\
  Chair of Statistics\\
  Location: Berlin\\
  Summer Term 2017\\
  Lecture: Einführung in die Bayes-Statistik\\
  Examiner: Dr. Florian Meinfelder
\end{raggedright}

\vspace*{\fill}
  \begin{center}
    \textbf{\Large{Program leave-one-out posterior predictive checking in R}}%
  \end{center}
\vfill

\begin{raggedleft}
  Johannes Brinkmann (), \href{mailto:jojo-brinkmann@gmx.de}{jojo-brinkmann@gmx.de}\\
  Carlo Michaelis (5043128), \href{mailto:carlo.michaelis@gmail.com}{carlo.michaelis@gmail.com}\\
  Max Reinhardt (579174), \href{mailto:maxreinhardt@me.com}{max\_reinhardt@me.com}\\
  Adrian Rolf (), \href{mailto:adrian.rolf@gmx.de}{adrian.rolf@gmx.de}\\
  \ \\
  Master Statistics\\
  \today\\
\end{raggedleft}

\newpage

<!-- Table of Contents -->
\tableofcontents
\newpage

<!-- List of Figures and Tables -->
\listoffigures
\listoftables
\newpage

<!-- Document Body -->
# Introduction

Leave-on-out cross validation is one of many methods which is used after fitting a bayesian model to measure its predictive accuracy and also to chose the best model [@vehtari2017practical]. In practice it was not often used in the past due to the fact that it involves additional computation power and time as different models must be fitted in order to find the best model.  
But there was a change in the last 10 to 15 years as computational power got cheaper and was therefore more available for the public [@hoeting1998bayesian].  
In the following paper model evaluation is done by calculating the mean squared error of the model as well as the log posterior predictive density. The best model is then selected by looking for the smallest values of those calculated parameters. In the second part of the model evaluation Cook’s distance is calculated by using two different methods for a comparison. For method one we use the leave-on-out cross validation bayesian approach and compare it with method two the analytic solution for a frequentist approach.

# Theory

## Bayesian Linear Model

We want to express the conditional value of $\mathbf{y}$ given $\mathbf{X}$ as a linear function of the nxk-matrix $\mathbf{X}$, where k is the number of independent variables included.  
This may be written as:

\begin{equation}
	\mathbf{y}=\mathbf{X}\beta + \epsilon 
\end{equation}

If we assume normally distributed errors $\epsilon$ this yields the following Likelihood function:

\begin{equation}
	L(\beta, \sigma^2, \mathbf{y}, \mathbf{X}) =  p(\mathbf{y}|\beta, \sigma^2, \mathbf{X}) = \Big(\frac{1}{2\pi{\sigma^2}}\Big) e^{-\frac{1}{2\sigma^2}(\mathbf{y}-\mathbf{X}\beta)'(\mathbf{y}-\mathbf{X}\beta)}
\end{equation}

Solving for $\beta$ would yield the frequentist OLS-estimator $\hat{\beta} = \mathbf{(X'X)^{-1}X'y}$. However, in a Bayesian approach we are not so much concerned with finding specific values for the coefficient vector $\beta$, but rather being able to comment on the probability distribution of $\beta$. Normalising for $(\beta-\hat{\beta})$ gives us:

\begin{equation} \label{likeli} \setlength{\jot}{10pt}
\begin{aligned}
	p(\mathbf{y}|\beta, \sigma^2, \mathbf{X}) {} & = \big(\frac{1}{2\pi}\big)^{\frac{n}{2}} \sigma^{-n} e^{-\frac{1}{2\sigma^2}\big[vs^2+(\beta-\hat{\beta})'\mathbf{X'X}(\beta-\hat{\beta})\big]} \\
	& \propto{\sigma^{-n}} e^{-\frac{vs^2}{2\sigma^2}} e^{-\frac{1}{2\sigma^2}\big[(\beta-\hat{\beta})\mathbf{X'X}(\beta-\hat{\beta})\big]} \\
\end{aligned}
\end{equation}

where $vs^2=(\mathbf{y}-\mathbf{X}\hat{\beta})'(\mathbf{y}-\mathbf{X}\hat{\beta})$ and $v=n-k$.

Applying a prior distribution for $\beta$ and $\sigma^2$; $p(\beta, \sigma^2)$ yields the posterior distribution for $\beta$ and $\sigma^2$, which is a distribution that has been updated by the actual observed data represented by $\mathbf{y}$ and $\mathbf{X}$. In this case, the conjugate prior to a likelihood as seen in equation \ref{likeli} is an inverse-gamma distribution for $\sigma^2$. Thus, the prior would be of the type $p(\beta, \sigma^2)=p(\beta|\sigma^2)p(\sigma^2)$, where $p(\sigma^2)$ is inverse-gamma distributed. Using Bayes' Theorem, the posterior distribution may now be derived by solving the following term:

\begin{equation}
	p(\beta, \sigma^2 | \mathbf{y, X}) \propto p(\mathbf{y} | \beta, \sigma^2, \mathbf{X}) p(\beta|\sigma^2) p(\sigma^2) 
\end{equation}

## Leave-one-out cross-validation

\textit{Leave-one-out cross-validation}, or short \textit{loo-cv} is a method for checking the predictive out-of-sample accuracy of a model. The idea is to omit the $i$-th observation and to fit the model for the remaining observations and then to predict the $y_i$  as an out of sample point. This prediction is denoted as $\hat{y}_i^{(i)}$. Thus, we can overcome -- at least to some extent -- the problem of overfitting. The loo-cv method can be used in several ways for model evaluation and selection. We will use it in two ways. First, we will use it to calculate the \textit{log pointwise prediction density} using the loo-cv, which is defined as:

\begin{equation}
\label{eq:lppd}
	\text{lppd}_{\text{loo-cv}} = \sum\limits_{i=1}^{n} \log p_{\text{dens}(-i)}(y_i).
\end{equation}

We first calculate the densities of all $y_i$ using the parameters we obtain from the $n$ bayesian models, applying the loo-cv method and then taking the logarithm and summing up. In our case we deal with the normal density, since $y_i \sim \mathcal{N}\left(X_{(-i)}\beta_{(-i)}, \sigma^2_{(-i)}\right)$. Furthermore, multiplying with $-2$, i.e. $-2\ \text{lppd}_{\text{loo-cv}}$, gives us a measure that is comparable with other measures of predictive accuracy (e.g. AIC or WAIC) (Gelman, 2013). Within the Bayesian framework, we will use $-2\ \text{lppd}_{\text{loo-cv}}$ and the MSE for model selection.



Second, we will need it in the context of comparing the predictive out-of-sample accuracy of the bayesian and the frequentist model using Cook's distance.


## Cooks's Distance

\textit{Cooks's Distance}, or short \textit{Cook's D}, is a measure to detect influential points in a linear regression. It was proposed by Cook (1977) and is defined as:

\begin{equation}
\label{eq:cookd}
	D_i = \frac{\sum_{j=1}^{n}\left(\hat{y}_j - \hat{y}_j^{(i)}\right)^2}{p\ \text{MSE}} \stackrel{\text{(OLS)}}=
	\frac{e_i^2}{p\ \text{MSE}}\left(\frac{h_{ii}}{(1-h_{ii})^2}\right),
\end{equation}

where

\begin{equation}
\label{eq:mse}
  \text{MSE}=\frac{1}{n-p}\sum_{i=1}^{n}(y_i-\hat{y})^2
\end{equation}

is the \textit{mean squared error}, $e_i^2 = (y_i-\hat{y})^2$ is the error for each observation $i$ and 
$h_{ii} = x_i(X^TX)^{-1}x_i^T$ is the $i$-th diagonal element of the projection matrix of an OLS regression model, also called the \textit{leverage} of an observation, which indicates the influence of an single observation on the model fit. 

As above, the $\hat{y}^{(i)}$ is the predicted value for $y_i$ if we exclude the $i$-th observation and fit the model for the remaining observations. 
Note that the right hand side term of the equation \eqref{2.2} is an analytical expression. In this case there is no need to fit additional $n$ models to get the $\hat{y}^{(i)}$ predictions. This yields to a great advantage in performance, especially for large numbers of observations. But the analytical expression is restricted to OLS models -- indicated by the \textit{OLS} over the equation sign -- since the projection matrix is used. For this reason we will use the left hand expression to calculate Cook's D in the Bayesian case using the loo-cv method described above and the right hand expression for the frequentist OLS regression model.

# Code

For both applications of the cross validation, the model evaluation and the calculation of Cook's distance, we need the Gibbs sampler and the leave-one-out cross validation (loo-cv) algorithm. These two functions are explained in the first part. The model evaluation is presented in the second part and the third part compares the loo-cv Cook's distance with the analytic solution, which are defined in equation \eqref{eq:cookd}.

Note that we only displayed the relevant parts of the code in the `pdf` file. Some sections regarding the plotting are hidden and can be found in the `rmd` file.

## Basics

The first function we define ist the *Gibbs sampler*. The Gibbs sampler draws the parameters $\beta_0, ..., \beta_p, \sigma^2$ from their posterior distributions. These distributions (see equation \eqref{??} and \eqref{??}) need the design matrix `X` and labels `Y`, which are given as arguments. Furthermore the Gibbs sampler does not just draw one time, it draws $B$ times, where $B = R + b$. $b$ is the number of burn in samples and $R$ the number of used samples. If $b = 0$, we use all generated samples.

In figure \ref{fig:traces} the traces of the Gibbs sampler are plotted for all parameters. It can be seen that the values drawn by the sampler differ in some range. Moreover we can see that the initial value of $\sigma^2 = 1$ is corrected very fast to a specific range. The initial value of $\beta$ is directly drawn in the range where most values are generated.

Furthermore in figure \ref{fig:densities} the histogram and the extimated density (using kernel method) is plotted for every parameter. The red line marks the posterior mean point estimator for any parameters.

Both figures are based on data for the whole dataset of the optimal model (see below).

```{r gibbsSampler}
gibbsSampler <- function(X, Y, R, b = 0, initBetas = 1) {
  # Sample parameters from posterior distribution
  # Prepared for baysean linear regression with ordinary least square approach (OLS)
  #
  # Args:
  #   X: Design matrix
  #   Y: Labels
  #   R: Number of draws (without burn in), R is a scalar in this funcrion
  #   b: Number of burn in draws
  #   initBetas: Initial value of betas (default 1)
  #
  # Returns:
  #   List containing sampled betas (number depends on design matrix) and sigmas
  
  # Get number of overall draws (including burn in)
  B <- R + b
  
  # Size of design matrix
  n <- nrow(X) # Number of data points
  p <- ncol(X) # Number of parameters
  
  # Variables to store the samples in
  betas <- matrix(nrow = B, ncol = p)
  sigma <- rep(NA, B)
  
  # OLS of beta
  V <- solve(t(X)%*%X)     # (X^T X)^-1
  beta_hat <- V%*%t(X)%*%Y # (X^T X)^-1 X^T Y
  
  # Initalize betas and sigma
  betas[1,] <- 1
  #betas[1,] <- beta_hat
  sigma[1] <- 1
  
  # Sampling
  for(i in 1:(B-1)){
    # Sample sigma from the full conditional
    counter <- t(Y-X%*%betas[i,])%*%(Y-X%*%betas[i,])
    sigma[i+1] <- 1/rgamma(1, n/2, counter/2)
    
    # Sample beta from the full conditional 
    if(i < B) {
      betas[i+1,] <- rmvnorm(1, beta_hat, sigma[i+1]*V)
    }
  }
  
  # Remove burn in, if there is some
  if(b != 0) {
    betas <- betas[-(1:b),]
    sigma <- sigma[-(1:b)]
  }
  
  return(list(betas = betas, sigma = sigma))
}
```

Next, we define a function for the leave-one-out cross validation. It deletes one data point and samples the parameters from the posterior distribution (using the `gibbsSampler` function). This is repeated $n$ times, where $n$ is the size (number of rows) of the data matrix, so there is a sampled posterior distribution for $n$ combinations of the data. Additionally the function estimates the posterior mean (point estimation) of the parameters and predicts the labels, using the data matrix $X$ and the point estimators of the parameters.

Note that the argument $R$ can be a scalar or a vector. If $R$ is a scalar, the crossvalidation only runs once. If $R$ is an ordered vector the Gibbs sampler draws as often as the length of vector $R$. The length of vector R is saved in the variable steps. Afterwards the calculation of the posterior mean and the prediction is done for every step in the vector $R$. In the first step, just the first $R_1$ number of samples are used to estimate the parameters. In the second step, $R_2 > R_1$ number of samples are used, and so on.

```{r crossValidation}
crossValidation <- function(X, Y, R, b) {
  # Implementation of leave-one-out cross validation (LOO-CV)
  # Cross validate bayesian linear regression model with OLS
  #
  # Args:
  #   X: Design matrix
  #   Y: Labels
  #   R: Number of draws (without burn in), R can be a vector (ordered) in this function
  #   b: Number of burn in draws
  #
  # Returns:
  #   List containing estimated parameters and label predictions for every LOO-CV step
  
  # Size of design matrix
  n <- nrow(X) # Number of data points 
  p <- ncol(X) # Number of parameters
  
  # Get size of R vector (steps = 1, if R is scalar)
  steps <- length(R)
  
  # Run gibbs sampler to get sampled parameters
  samples <- lapply(1:n, function(i) gibbsSampler(X[-i,], Y[-i], R[steps], b))
  
  # Initalize lists to store results of estimation and prediction
  Sigma <- list()
  Betas <- list()
  Yhati <- list()
  
  # Calculate sigma, betas and Yhati for every R step (do it once if R is scalar)
  for(k in 1:steps) {
    Sigma[[k]] <- sapply(samples, function(sample) mean(sample$sigma[1:R[k]]))
    Betas[[k]] <- sapply(samples, function(sample) colMeans(sample$betas[1:R[k],]))
    Yhati[[k]] <- sapply(1:n, function(i) X[i,]%*%Betas[[k]][,i])
  }
  
  return(list(Sigma = Sigma, Betas = Betas, Yhati = Yhati))
}
```

## Model evaluation
The cross validation algorithm can be used to evaluate the best model. The best mode is found by looking for the smallest values compared to other models using the mean squared error, defined in equation \eqref{eq:mse} or the *lppd* , defined in equation \eqref{eq:lppd}. Both measures are obtained by loo-cv.

The `bayesModelEvaluation` function calls the cross validation function for every model. Then for every model the *mse* and *lppd* is calculated and returned.

```{r modelEvaluationFunction}
bayesModelEvaluation <- function(models, Y, R, b) {
  # Model evaluation based on mean squared error (MSE) and log posterior
  # predictive density (lppd) using leave-one-out cross validation (LOO-CV)
  #
  # Args:
  #   models: A list of design matrices with different size and types of parameters
  #   Y: Labels
  #   R: Number of draws (without burn in), R can be a vector in this function
  #   b: Number of burn in draws
  #
  # Returns:
  #   List containing MSEs and LPPDs for every model
  
  # Evaluate multiple models and return results from all models
  n <- length(Y)
  k <- length(models)
  
  # Cross validate every model
  results <- lapply(1:k, function(i) crossValidation(models[[i]], Y, R, b))
  
  # Calculate Mean Squared Errors
  MSEs <- sapply(results, function(el) {
    return((1/(n-nrow(el$Betas[[1]])))*sum((Y-el$Yhati[[1]])^2))
  })
  
  # Calculate log posterior predictive density (log likelihood)
  # y ~ N(XB, s^2(X^T X)^-1) = N(Yhat, s^2(X^T X)^-1)
  LPPDs <- sapply(results, function(eva) sum(log(dnorm(Y, eva$Yhati[[1]], eva$Sigma[[1]]))))
  
  return(list(MSEs = MSEs, LPPDs = LPPDs))
}
```

To run the mode, we first used the `swiss` data set. We defined a list of 11 design matrices, containing different models. Afterwards the `bayesModelEvaluation` is called to run the cross validation and the calculation of *mse* and *lppd* for every model.

```{r modelEvaluation}
# Swiss data
dat <- swiss

# Response variable
Y <- dat$Fertility
n <- nrow(dat)

# Design matrices
models <- list(
  matrix(c(rep(1,n), dat$Education), nrow=n),
  matrix(c(rep(1,n), dat$Agriculture), nrow=n),
  matrix(c(rep(1,n), dat$Examination), nrow=n),
  matrix(c(rep(1,n), dat$Catholic), nrow=n),
  matrix(c(rep(1,n), dat$Education, dat$Agriculture), nrow=n),
  matrix(c(rep(1,n), dat$Education, dat$Examination), nrow=n),
  matrix(c(rep(1,n), dat$Education, dat$Catholic), nrow=n),
  matrix(c(rep(1,n), dat$Education, dat$Agriculture, dat$Examination), nrow=n),
  matrix(c(rep(1,n), dat$Education, dat$Agriculture, dat$Catholic), nrow=n),
  matrix(c(rep(1,n), dat$Education, dat$Examination, dat$Catholic), nrow=n),
  matrix(c(rep(1,n), dat$Education, dat$Agriculture, dat$Examination ,dat$Catholic), nrow=n)
)

# Run model evaluation
criteria <- bayesModelEvaluation(models, Y, R = 50, b = 10) # R = 500, b = 100
```

```{r plotModelEvaluationResults, echo=FALSE, fig.cap="\\label{fig:evalu}Model evaluation with leave-one-out cross validation (LOO-CV) using mean squared errors (MSE) and log posterior predictive density (lppd)to obtain the best model.", fig.scap="Model evaluation with MSE and lppd", fig.align="center"}
par(mfrow = c(1,2))
# Plot MSEs
barplot(criteria$MSEs, xlab = "Models", ylab = "MSE", names.arg = seq(1,length(models)),
        main = "Model evaluation using MSE")

# Plot LPPDs
barplot(-2*criteria$LPPDs, xlab = "Models", ylab = "lppd", names.arg = seq(1,length(models)),
        main = "Model evaluation using lppd")
```

The results in figure \ref{fig:evalu} show that model $9$ seems to be the best model with the lowest *mse* and lowest the *lppd*, compared to the other 10 models. To check this quantitatively we get the minimum of the *mse* values and the minimum of the *lppd* values.

```{r}
print(which.min(criteria$MSEs))
print(which.min(-2*criteria$LPPDs))
```

Indeed, both indicators favor model $9$. We now that according to Gelman (2013), *mse* is just appropriate if the model is more or less normally distributed. Furthermore if the models are normal and have constant variances, *mse* and *lppd* should have the same proportion.

The proportions are calculated in table 1. It can be seen, that the proportion is in the same scale in all cases, but there are deviations. For exmaple in model $2$ and $9$ the proportions are very different. This is probably due to the fact, that all the models are not well normally distributed.

```{r evaluateModelEvaluationTable, results="asis", echo=FALSE}
# Check proportion
prop <- matrix(criteria$MSEs/(-2*criteria$LPPDs), ncol = length(models),
               dimnames = list("proportion", seq(1,length(models))))
print(xtable(prop, caption = "Proportion of mse and lppd."), comment = FALSE)
```

Finally we store the index of the optimal model in a variable. In the next step we want to calculate Cook's distance just for the best mode.

```{r chooseOptimal}
# Choose optimal modal
optIdx <- which.min(-2*criteria$LPPDs)
```

## Cook's Distance

```{r plotTraceFunction, echo = FALSE}
plotTraces <- function(sample) {
  # Uses a sample from Gibbs sampler to plot traces of drawn parameters
  # -> Burn in samples are shown
  #
  # Args:
  #   sample: A sample from Gibbs sampler
  #
  # Returns:
  #   -
  
  # Get number of available steps, but max 1000
  numTraceSteps <- min(c(nrow(sample$betas), 1000))
  
  # Prepare plot frame
  q <- ncol(sample$betas) + 1
  frameRows <- round(q/2+0.1)
  par(mfrow = c(frameRows,2))
  
  # Plot traces
  for(i in 1:ncol(sample$betas)) {
    plot(sample$betas[1:numTraceSteps,i], type='l', ylab=bquote(beta[.(i-1)]),
         main=bquote("Trace of" ~ beta[.(i-1)]))
  }
  plot(sample$sigma[1:numTraceSteps], type='l', ylab=bquote(sigma^2),
       main=bquote("Trace of" ~ sigma^2))
}
```

```{r plotDensityFunction, echo = FALSE}
plotDensities <- function(sample, b) {
  # Uses a sample from Gibbs sampler to plot marginal posterior densities
  # of drawn parameters, where densities are estimated using a kernel estimation
  # -> Burn in samples are removed
  #
  # Args:
  #   sample: A sample from Gibbs sampler
  #   b: Number of burn in draws
  #
  # Returns:
  #   -
  
  # Function to draw plot
  drawHistDensity <- function(para, para_name, para_main) {
    # Local helper function to plot histogram with density esimtation and posterior mean
    #
    # Args:
    #   para     : Parameter (e.g. Beta, Sigma)
    #   para_name: Name of parameter
    #   para_main: Main title of plot
    #
    # Returns:
    #   -
    
    # Estimate density for parameter values
    density <- density(para, bw = "SJ")
    
    # Draw histogram and add estimated density line
    hist(para, freq = FALSE, ylim = c(0,max(density$y)),
         xlab = para_name, ylab=NULL, main = para_main)
    lines(density, col="blue")
    
    # Posterior mean
    abline(v = mean(para), col="red")
  }
  
  # Prepare plot frame
  q <- ncol(sample$betas) + 1
  frameRows <- round(q/2+0.1)
  par(mfrow = c(frameRows,2))
  
  # Plot histogram, densities and posterior mean (remove burn in samples)
  for(i in 1:ncol(sample$betas)) {
    drawHistDensity(sample$betas[-(1:b),i], bquote(beta[.(i-1)]),
                    bquote("Marginal posterior density of" ~ beta[.(i-1)]))
  }
  drawHistDensity(sample$sigma[-(1:b)], bquote(sigma),
                  bquote("Marginal posterior density of" ~ sigma))
}
```

The second application of the cross validation is the approximate calculation of Cook's distance, which ist mathematically defined in equation \eqref{eq:cookd}. The function `cooksDistance` calculates the approximate solution, using loo-cv, and the analytic solution, based on frequentist linear regression with ordinary least squares.

To calculate the approximation of Cook's distance with cross validation we make use of the implemented `crossValidation` function above. We choose `R` as a vector, such that the cross validation function calculates multiple estimates for many different sample sizes. Finally we just apply the formula for Cook's distance.

For the analytic solution, we calculate a frequentist linear regression, where `\hat\beta` was solved by ordinary least square approach. After we obtain the point estimation, we can calculate Cook's distance.

In the end a list is returned, containing the analytic solution as a vector and the loo-cv solutions as a matrix, where the number of columns depend on the size of vector `R`.

```{r cooksDistanceFunction}
cooksDistance <- function(X, Y, R, b) {
  # Calculate cooks distances for optimal model from model evaluaion
  # Two methods are used:
  #   (1) Use LOO-CV approach for bayesian linear regression model with OLS
  #   (2) Use analytic solution for frequentist linear regression model with OLS
  #
  # Args:
  #   X: Design matrix
  #   Y: Labels
  #   R: Number of draws (without burn in), R can be a vector in this function
  #   b: Number of burn in draws
  #
  # Returns:
  #   cooksBayesCV:   Cook's distances calculated by LOO-CV (bayes model),
  #                   where cook's distances are calculated for different sample sizes
  #   cooksAnalytic:  Cook's distances calculated by analytic solution
  #   sample:         The gibbs sample from the bayes linear model (for plotting purpose)
  
  # R is usually a vector (but don't has to)
  B <- R + b
  steps <- length(R)
  
  # Prepare projection matrix and number of parameters
  H <- X%*%solve(t(X)%*%X)%*%t(X)
  p <- ncol(X)
  
  # Run cross validation with vector R
  cv <- crossValidation(X, Y, R, b)
  
  # Sample whole model (add B = R + b samples, remove b later)
  sample <- gibbsSampler(X, Y, R[steps] + b)
  
  # Cook's distance: Frequentist appraoch with analytic solution
  betaHat <- solve(t(X)%*%X)%*%t(X)%*%Y
  YhatLinReg <- X%*%betaHat
  E <- Y-YhatLinReg
  cooksAnalytic <- (E^2/((1/(n-p))*sum(E^2)*p))*(diag(H)/(1-diag(H))^2)
  
  # Cook's distance: Bayesian appraoch with LOO-VC solution
  cooksBayesCV <- matrix(nrow = n, ncol = steps)
  for(k in 1:steps) {
    # Estimate posterior mean from betas
    betas <- colMeans(sample$betas[b:B[k],])
    
    # Predict values, using posterior mean
    Yhat <- X%*%betas
    
    # Calculate cook's distance
    dists <- apply(cv$Betas[[k]], 2, function(betas) {
      return((Yhat - X%*%betas)^2)
    })
    mse <- (1/(n-p))*sum((Y-Yhat)^2)
    cooksBayesCV[,k] <- colSums(dists)/(p*mse)
  }
  
  return(list(cooksBayesCV = cooksBayesCV, cooksAnalytic = cooksAnalytic, sample = sample))
}
```

We apply the function above to the optimal model, obtained by the model evaluation. We choose a specific sequence of sample sizes `R` and a burn in size.

```{r cooksDistance}
# Number of samples
b <- 100
R <- seq(100,500,50) # seq(100,5000,50)

# Get optimal model and calculate projection matrix
cooks <- cooksDistance(models[[optIdx]], Y, R, b)
```

```{r plotTracesExample, echo=FALSE, fig.height=8, fig.cap="\\label{fig:traces}Traces of sampled parameters from posterior distributions regarding bayesian linear regression with ordinary least squares.", fig.scap="Traces of sampled parameters from posterior distributions", fig.align="center"}
# Plot traces
plotTraces(cooks$sample)
```

```{r plotDensitiesExample, echo=FALSE, fig.height=8, fig.cap="\\label{fig:densities}Posterior densities, histograms and posterior means (red vertical line) of sampled parameters regarding bayesian linear regression with ordinary least squares", fig.scap="Posterior densities of sampled parameters", fig.align="center"}
# Plot traces
plotDensities(cooks$sample, b)
```

In the first step, we want to have a look at Cook's distances. In figure \ref{fig:cookd} the blue bars in all 3 plots are always the same analytic solution. They are compared to the the loo-cv solution using Bayes approach, which is shown in red. In the first plot `r R[1]` simulations were conducted, in the second plot `r R[round(length(R)/5)]` and `r R[length(R)]` in the last plot. We can observe, that the more simulation are done the loo-cv solution is coming closer to the analytic solution.

```{r plotCookDistances, echo=FALSE, fig.height=10, fig.cap="\\label{fig:cookd}Pointwise cook's distance, calculated with three different sample sizes, compared to analytic cook's distance from frequentist linear regression model using least squares.", fig.scap="Pointwise cook's distance comparison between COO-LV and analytic solution", fig.align="center"}
# Plot Cook's distances pointwise for 3 sample sizes
par(mfrow = c(3,1))
sampleSizeToPlot <- c(1, round(length(R)/5), length(R))
for(i in sampleSizeToPlot) {
  barplot(t(cbind(cooks$cooksBayesCV[,i], cooks$cooksAnalytic)), beside = TRUE, names.arg = seq(1:n),
          legend.text = c("Bayes: cross validation", "Frequentistic: analytic"),
          args.legend = list(x = 'topleft', bty='n'), col = rainbow(2, s=0.5),
          xlab = "Data point", ylab = "Cook's distance",
          main=paste("Cook's distance comparison after", R[i], "simulations"))
}
```

To see the convergence behavior of the loo-cv solution to the analytic solution, we developed a measure $d$ caluclating the distance between both solutions. The Cook's distance value for both solutions were subtracted, squared and summed (squared distance) as follows

\begin{equation}
d = \sum_{i=1}^n (D^\text{loo-cv}_i - D^\text{analytic}_i)^2,
\end{equation}

where $n$ is the number of data points, $D^\text{loo-cv}_i$ Cook's distance for the loo-cv solution and $D^\text{analytic}_i$ Cook's distance for the analytic solution for point $X_i$.

```{r prepareCookDistancesPlot}
# Initialize matrices
steps <- length(R)
matBayesD <- cooks$cooksBayesCV
matFreqD <- matrix(rep(cooks$cooksAnalytic, steps), ncol=steps)

# Calculate differences between Cook's distance methods
cooksMeasure <- colSums((matBayesD-matFreqD)^2)
```

In figure \ref{fig:cookdmeasure} we can see that the defined distance $d$ becomes lower. In other words: both solutions are coming closer together the more simulations are done. Therefore a convergence behavior can be seen.

```{r plotCookDistancesMeasure, echo=FALSE, fig.cap="\\label{fig:cookdmeasure}Cook's distance measure to quantify approximation from cross validated cook's distances of analytic cook's distances. The approximate solution with LOO-CV comes closer to the analytic solution, the more samples are drawn.", fig.scap="Cook's distance measure for approximation of analytic solution by LOO-CV solution", fig.align="center"}
# Plot differences between Cook's distance for different sample sizes
par(mfrow = c(1,1))
barplot(cooksMeasure, xlab="Sample size", ylab="Difference measure", names.arg=R,
        main="Square distances: Cook's cross validation vs. Cook's analytic")
```

<!-- References -->
# References

