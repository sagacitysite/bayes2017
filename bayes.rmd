---
output: 
  pdf_document:
    fig_caption: true
    keep_tex: true
    latex_engine: pdflatex
    number_sections: true
    fig_height: 3
    fig_width: 10
header-includes:
- \usepackage[english]{babel}
- \usepackage{setspace}
- \usepackage{bm}
- \hyphenation{}
- \pagenumbering{arabic}
- \numberwithin{equation}{section}
- \newcommand{\E}{\mathbb{E}}
- \newcommand{\R}{\mathbb{R}}
- \newcommand{\argmax}{\text{argmax}}
fontsize: 10pt
geometry: a4paper, left=30mm, right=30mm, top=20mm, bottom=20mm, includefoot
bibliography: references.bib
documentclass: article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, echo=FALSE, warning=FALSE, message=FALSE}
#library(MASS)
#library(mixtools)
#library(plyr)
#library(xtable)
library(mvtnorm)
```

<!-- Titlepage -->
\thispagestyle{empty}

\begin{raggedright}
  Freie Universität Berlin\\
  Chair of Statistics\\
  Location: Berlin\\
  Summer Term 2017\\
  Lecture: Einführung in die Bayes-Statistik\\
  Examiner: Dr. Florian Meinfelder
\end{raggedright}

\vspace*{\fill}
  \begin{center}
    \textbf{\Large{Program leave-one-out posterior predictive checking in R}}%
  \end{center}
\vfill

\begin{raggedleft}
  Johannes Brinkmann (), \href{mailto:jojo-brinkmann@gmx.de}{jojo-brinkmann@gmx.de}\\
  Carlo Michaelis (5043128), \href{mailto:carlo.michaelis@gmail.com}{carlo.michaelis@gmail.com}\\
  Max Reinhardt (), \href{mailto:maxreinhardt@me.com}{max\_reinhardt@me.com}\\
  Adrian Rolf (), \href{mailto:adrian.rolf@gmx.de}{adrian.rolf@gmx.de}\\
  \ \\
  Master Statistics\\
  \today\\
\end{raggedleft}

\newpage

<!-- Table of Contents -->
\tableofcontents
\newpage

<!-- List of Figures and Tables -->
\listoffigures
\listoftables
\newpage

<!-- Document Body -->
# Introduction

# Code

## General code

The Gibbs sampler

```{r gibbsSampler}
gibbsSampler <- function(X, Y, R, b = 0, initSigma = 1) {
  B <- R + b
  
  # Size of design matrix
  n <- nrow(X)
  p <- ncol(X)
  
  # Variables to store the samples in (initalize sigma with initSigma)
  betas <- matrix(nrow = B, ncol = p)
  sigma <- c(initSigma, rep(NA, B-1))
  
  # Sampling
  for(i in 1:B){
    # OLS of beta
    V <- solve(t(X)%*%X)     # (X^T X)^-1
    beta_hat <- V%*%t(X)%*%Y # (X^T X)^-1 X^T Y
    
    # OLS of sigma
    sigma_hat <- t(Y-X%*%beta_hat)%*%(Y-X%*%beta_hat)/(n-p)
    
    # Sample beta from the full conditional 
    betas[i,] <- rmvnorm(1, beta_hat, sigma[i]*V)
    
    # Sample sigma from the full conditional
    if(i < B) {
      sigma[i+1] <- 1/rgamma(1, (n-p)/2, (n-p)*sigma_hat/2)
    }
  }
  
  # Remove burn in
  if(b != 0) {
    betas <- betas[-(1:b),]
    sigma <- sigma[-(1:b)]
  }
  
  return(list(betas = betas, sigma = sigma))
}
```

Cross Validation function

```{r crossValidation}
crossValidation <- function(X, Y, R, b) {
  # Size of design matrix
  n <- nrow(X)
  p <- ncol(X)
  steps <- length(R)
  
  # Run gibbs sampler to get sampled parameters
  samples <- lapply(1:n, function(i) gibbsSampler(X[-i,], Y[-i], R[steps], b))
  
  # Initalize lists
  Sigma <- list()
  Betas <- list()
  Yhati <- list()
  
  # Calculate sigma, betas and Yhati for every step
  for(k in 1:steps) {
    Sigma[[k]] <- sapply(samples, function(sample) mean(sample$sigma[1:R[k]]))
    Betas[[k]] <- sapply(samples, function(sample) colMeans(sample$betas[1:R[k],]))
    Yhati[[k]] <- sapply(1:n, function(i) X[i,]%*%Betas[[k]][,i])
  }
  
  return(list(Sigma = Sigma, Betas = Betas, Yhati = Yhati))
}
```

## Model evaluaion

```{r modelEvaluationFunction}
bayesModelEvaluation <- function(models, Y, R, b) {
  # Evaluate multiple models and return results from all models
  n <- length(Y)
  k <- length(models)
  
  # Cross validate every model
  results <- lapply(1:k, function(i) crossValidation(models[[i]], Y, R, b))
  
  # Calculate Mean Squared Errors
  MSEs <- sapply(results, function(el) {
    return((1/(n-nrow(el$Betas[[1]])))*sum((Y-el$Yhati[[1]])^2))
  })
  
  # Plot MSEs
  par(mfrow = c(1,2))
  barplot(MSEs, xlab = "Models", ylab = "MSE", names.arg = seq(1,k),
          main = "Model evaluation using MSE")
  
  # Calculate log posterior predictive density (log likelihood)
  # y ~ N(XB, s^2(X^T X)^-1) = N(Yhat, s^2(X^T X)^-1)
  LPPDs <- sapply(results, function(eva) sum(log(dnorm(Y, eva$Yhati[[1]], eva$Sigma[[1]]))))
  
  # Plot LPPDs
  barplot(-2*LPPDs, xlab = "Models", ylab = "lppd", names.arg = seq(1,k),
          main = "Model evaluation using lppd")
  
  return(list(MSEs = MSEs, LPPDs = LPPDs))
}
```

Run model evaluation with data

```{r modelEvaluation, fig.height=5}
# Swiss data
dat <- swiss

# Response variable
Y <- dat$Fertility
n <- nrow(dat)

# Design matrices

models <- list(
  matrix(c(rep(1,n), dat$Education), nrow=n),
  matrix(c(rep(1,n), dat$Agriculture), nrow=n),
  matrix(c(rep(1,n), dat$Examination), nrow=n),
  matrix(c(rep(1,n), dat$Catholic), nrow=n),
  matrix(c(rep(1,n), dat$Education, dat$Agriculture), nrow=n),
  matrix(c(rep(1,n), dat$Education, dat$Examination), nrow=n),
  matrix(c(rep(1,n), dat$Education, dat$Catholic), nrow=n),
  matrix(c(rep(1,n), dat$Education, dat$Agriculture, dat$Examination), nrow=n),
  matrix(c(rep(1,n), dat$Education, dat$Agriculture, dat$Catholic), nrow=n),
  matrix(c(rep(1,n), dat$Education, dat$Examination, dat$Catholic), nrow=n),
  matrix(c(rep(1,n), dat$Education, dat$Agriculture, dat$Examination ,dat$Catholic), nrow=n)
)

### Model evaluation ###
criteria <- bayesModelEvaluation(models, Y, R = 50, b = 10) # R = 500, b = 100

# Check proportion
print(criteria$MSEs/(-2*criteria$LPPDs))
# TODO: Als Tabelle ausgeben? Explain why not fulfilled

# Choose optimal modal
optIdx <- which.min(-2*criteria$LPPDs)

# Check if MSE and LPPD would choose the same
print(paste(which.min(criteria$MSEs), "=", which.min(-2*criteria$LPPDs)))
# TODO den Vergleich einfach nur im Text verwenden?
```

## Cook's Distance

```{r plotTraceExample, echo = FALSE}
plotTraces <- function(sample) {
  ## Traces
  
  # Get number of available steps, but max 1000
  numTraceSteps <- min(c(nrow(sample$betas), 1000))
  
  # Plot traces
  par(mfrow = c(1,2), cex=1)
  for(i in 1:ncol(sample$betas)) {
    plot(sample$betas[1:numTraceSteps,i], type='l', ylab=bquote(beta[.(i-1)]),
         main=bquote("Trace of" ~ beta[.(i-1)]))
  }
  plot(sample$sigma[1:numTraceSteps], type='l', ylab=bquote(sigma^2),
       main=bquote("Trace of" ~ sigma^2))
}
```

```{r plotDensityExample, echo = FALSE}
plotDensities <- function(sample, b) {
  ## Marginal posterior densities (remove burn in)
  
  # Function to draw plot
  drawHistDensity <- function(para, para_name) {
    # para     : Parameter (e.g. Beta, Sigma)
    # para_name: Title of plot
    
    # Estimate density for parameter values
    density <- density(para, bw = "SJ")
    
    # Draw histogram and add estimated density line
    hist(para, freq = FALSE, ylim = c(0,max(density$y)), xlab = para_name,
         ylab=NULL, main = "Marginal posterior density")
    lines(density, col="blue")
    
    # Posterior mean
    abline(v = mean(para), col="red")
  }
  
  # Adjust frame and plot all parameters
  par(mfrow = c(1,2), cex=1)
  for(i in 1:ncol(sample$betas)) {
    drawHistDensity(sample$betas[-(1:b),i], bquote(beta[.(i-1)]))
  }
  drawHistDensity(sample$sigma[-(1:b)], bquote(sigma))
}
```

```{r cooksDistanceFunction}
cooksDistance <- function(X, Y, R, b) {
  # R is a vector
  B <- R + b
  
  # Prepare projection matrix and number of parameters
  H <- X%*%solve(t(X)%*%X)%*%t(X)
  p <- ncol(X)
  steps <- length(R)
  
  # Run cross validation with vector R
  cv <- crossValidation(X, Y, R, b)
  
  # Sample whole model (add R+b samples, remove b later)
  sample <- gibbsSampler(X, Y, R[steps] + b)
  
  # Plot traces and densities
  plotTraces(sample)
  cat("\n\n") # Add some space to avoid floating
  plotDensities(sample, b)
  
  # Cook's distance: Frequentist appraoch with analytic solution
  betaHat <- solve(t(X)%*%X)%*%t(X)%*%Y
  YhatLinReg <- X%*%betaHat
  E <- Y-YhatLinReg
  cooksLinReg <- (E^2/((1/(n-p))*sum(E^2)*p))*(diag(H)/(1-diag(H))^2)
  
  cooksBayesCV <- matrix(nrow = n, ncol = steps)
  for(k in 1:steps) {
    # Estimate posterior mean from betas
    betas <- colMeans(sample$betas[b:B[k],])
    
    # Predict values, using posterior mean
    Yhat <- X%*%betas
    
    # Calculate cook's distance
    dists <- apply(cv$Betas[[k]], 2, function(betas) {
      return((Yhat - X%*%betas)^2)
    })
    mse <- (1/(n-p))*sum((Y-Yhat)^2)
    cooksBayesCV[,k] <- colSums(dists)/(p*mse)
  }
  
  return(list(cooksBayesCV = cooksBayesCV, cooksLinReg = cooksLinReg))
}
```

```{r cookDistancePlotFunction, echo = FALSE}
# Plot Cook's distance result
plotCooks <- function(cooks, R) {
  # Initialize matrices
  steps <- length(R)
  matBayesD <- cooks$cooksBayesCV
  matFreqD <- matrix(rep(cooks$cooksLinReg, steps), ncol=steps)
  
  # Calculate differences between Cook's distance methods
  cooksMeasure <- colSums((matBayesD-matFreqD)^2)
  
  # Plot differences between Cook's distance for different sample sizes
  par(mfrow = c(1,1))
  barplot(cooksMeasure, xlab="Sample size", ylab="Difference measure", names.arg=R,
          main="Square distances: Cook's cross validation vs. Cook's analytic")
  
  # Plot Cook's distances pointwise for 3 sample sizes
  par(mfrow = c(1,1))
  sampleSizeToPlot <- c(1, round(length(R)/5), length(R))
  for(i in sampleSizeToPlot) {
    barplot(t(cbind(cooks$cooksBayesCV[,i], cooks$cooksLinReg)), beside = TRUE, names.arg = seq(1:n),
            legend.text = c("Bayes: cross validation", "Frequentistic: analytic"),
            args.legend = list(x = 'topleft', bty='n'), col = rainbow(2, s=0.5),
            xlab = "Data point", ylab = "Cook's distance",
            main=paste("Cook's distance comparison after", R[i], "simulations"))
  }
}
```

```{r cooksDistance}
### Cook's Distance ###

# Number of samples
b <- 100
R <- seq(100,500,50) # seq(100,5000,50)

# Get optimal model and calculate projection matrix
cooks <- cooksDistance(models[[optIdx]], Y, R, b)
```

```{r plotCookDistances, fig.height=5}
# Plot results from Cook's distance calculation
plotCooks(cooks, R)
```

<!-- References -->
# References